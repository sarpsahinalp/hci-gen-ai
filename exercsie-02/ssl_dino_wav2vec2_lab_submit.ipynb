{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4f7042e",
   "metadata": {},
   "source": [
    "\n",
    "# Self-Supervised Learning Lab (2h): DINOv2 & Wav2Vec2‑BERT Embeddings + PCA\n",
    "**Runtime:** Jupyter with GPU • **Libraries:** PyTorch, torchvision, HuggingFace transformers, datasets, scikit-learn, matplotlib\n",
    "**Learning goals**\n",
    "- Understand the intuition behind self-supervised pretraining (contrastive, self-distillation).\n",
    "- Extract embeddings from a **vision SSL model** (DINOv2) and an **audio SSL model** (Wav2Vec2‑BERT).\n",
    "- Reduce embeddings with **PCA** and visualize the **first 3 components**.\n",
    "- Interpret what principal components capture for images (patch tokens) vs. audio (frame features).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735eed24",
   "metadata": {},
   "source": "## 0) Setup & Installs"
  },
  {
   "cell_type": "code",
   "id": "d364b077",
   "metadata": {},
   "source": [
    "\n",
    "# If running on a fresh environment, uncomment the next line to install requirements.\n",
    "!pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "!pip install --upgrade transformers datasets accelerate soundfile librosa scikit-learn matplotlib pillow timm\n",
    "\n",
    "# in your \"teaching\" env\n",
    "!pip install --upgrade \"transformers==4.43.3\"\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import torch, torchvision, transformers, datasets, sklearn, matplotlib, platform",
   "id": "57f0db874825b25f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(torch.__version__, torch.version.cuda, platform.platform())",
   "id": "7f421d86a8fd37bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "b4d5e27f3215a201"
  },
  {
   "cell_type": "markdown",
   "id": "c44e057f",
   "metadata": {},
   "source": [
    "### GPU Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "id": "242c79c6",
   "metadata": {},
   "source": [
    "\n",
    "# Quick CUDA test\n",
    "x = torch.randn(8192, 8192, device='cuda' if torch.cuda.is_available() else 'cpu')\n",
    "y = x @ x.T\n",
    "print(\"Matmul OK on\", y.device)\n",
    "del x, y\n",
    "torch.cuda.empty_cache()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "220b8ea9",
   "metadata": {},
   "source": [
    "\n",
    "## 1) SSL Intuition\n",
    "- **DINOv2**: self-distillation with no labels. Student network matches a teacher’s targets from multiple augmented views.\n",
    "- **Wav2Vec2‑BERT**: contrastive/masked prediction pretraining on raw audio; contextualized via Transformer layers.\n",
    "\n",
    "**Checkpoint Q1 (short answer, 2–3 sentences):**  \n",
    "Why do SSL models often yield features that transfer well with small labeled datasets?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d629bd3",
   "metadata": {},
   "source": [
    "## 2) Vision: DINOv2 Embeddings & PCA (35 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7de943",
   "metadata": {},
   "source": "\n### 2.1 Load Images\nYou can either:\n- Use the provided sample URLs, or\n- Replace with your own images (local paths).\n\n> **Note:** The sample set now includes a **cityscape**, a **human portrait**, and an **animal** photo (dog). Feel free to replace with your own URLs or local files.\n"
  },
  {
   "cell_type": "code",
   "id": "295f3c19",
   "metadata": {},
   "source": "\nfrom pathlib import Path\nfrom PIL import Image\nimport requests, io\n\n# Choose a few images (feel free to replace)\nIMAGE_URLS = [\n    \"https://images.unsplash.com/photo-1504196606672-aef5c9cefc92\",  # city\n    \"https://images.unsplash.com/photo-1529626455594-4ff0802cfb7e\",  # human\n    \"https://images.unsplash.com/photo-1517841905240-472988babdf9\",  # animal\n]\n\n\ndef load_image_from_url(url):\n    r = requests.get(url, timeout=10)\n    r.raise_for_status()\n    return Image.open(io.BytesIO(r.content)).convert(\"RGB\")\n\nimages = [load_image_from_url(u) for u in IMAGE_URLS]\nlen(images), images[0].size\n",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d03972ba",
   "metadata": {},
   "source": [
    "### 2.2 Load DINOv2 model & processor"
   ]
  },
  {
   "cell_type": "code",
   "id": "69b35434",
   "metadata": {},
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# You can pick a different variant: dinov2-small, base, large, giant\n",
    "\n",
    "image_processor = torch.hub.load('facebookresearch/dinov2', 'dinov2_vits14')\n",
    "vision_model = AutoModel.from_pretrained(MODEL_ID_VISION).to(device).eval()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "41c63f95178fc56b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e23c22e9",
   "metadata": {},
   "source": [
    "### 2.3 Extract Embeddings (CLS & Patch Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "id": "d5bf9c9d",
   "metadata": {},
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_dino_features(pil_img):\n",
    "    inputs = image_processor(images=pil_img, return_tensors=\"pt\").to(device)\n",
    "    outputs = vision_model(**inputs)\n",
    "    #TODO: write the complete function\n",
    "    # Many ViT-style models return last_hidden_state: [B, tokens, dim]\n",
    "    # CLS token is at index 0; patch tokens follow.\n",
    "    # [tokens, dim]\n",
    "    # [dim]\n",
    "    # [num_patches, dim]\n",
    "    return cls, patches\n",
    "\n",
    "vision_cls = []\n",
    "vision_patches = []\n",
    "for img in images:\n",
    "    c, p = get_dino_features(img)\n",
    "    vision_cls.append(c)\n",
    "    vision_patches.append(p)\n",
    "\n",
    "vision_cls = np.stack(vision_cls)   # [N_images, dim]\n",
    "print(\"CLS shape:\", vision_cls.shape)\n",
    "print(\"Patch tokens for img0:\", vision_patches[0].shape)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "126fad00",
   "metadata": {},
   "source": [
    "### 2.4 PCA → First 3 Components as RGB (per image)"
   ]
  },
  {
   "cell_type": "code",
   "id": "6ceeaa0e",
   "metadata": {},
   "source": [
    "\n",
    "import math\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def pca_to_rgb(features_2d):\n",
    "    # features_2d: [num_tokens, dim]\n",
    "    pca = PCA(n_components=3, random_state=0)\n",
    "    comps = pca.fit_transform(features_2d)  # [num_tokens, 3]\n",
    "    # normalize to [0,1] for display\n",
    "    comps = (comps - comps.min(0)) / (comps.ptp(0) + 1e-8)\n",
    "    return comps  # as \"RGB\" per token\n",
    "\n",
    "def tokens_to_grid(tokens, grid_hw=None):\n",
    "    # Infer a square-ish grid if not provided\n",
    "    if grid_hw is None:\n",
    "        L = tokens.shape[0]\n",
    "        h = w = int(math.sqrt(L))\n",
    "        if h * w != L:\n",
    "            # fallback to nearest rectangle (e.g., 14x16 for L=224)\n",
    "            for hh in range(h, h+10):\n",
    "                if L % hh == 0:\n",
    "                    h = hh; w = L // hh; break\n",
    "        return tokens.reshape(h, w, -1), (h, w)\n",
    "    else:\n",
    "        h, w = grid_hw\n",
    "        return tokens.reshape(h, w, -1), (h, w)\n",
    "\n",
    "for idx, (img, patch_feats) in enumerate(zip(images, vision_patches)):\n",
    "    rgb = pca_to_rgb(patch_feats)                 # [num_patches, 3]\n",
    "    grid, (H, W) = tokens_to_grid(rgb)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.title(f\"DINOv2 Patch PCA RGB (Image {idx})\")\n",
    "    plt.imshow(grid)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f453c73a",
   "metadata": {},
   "source": [
    "### 2.5 2D Scatter of Patch Tokens (PCA 2D)"
   ]
  },
  {
   "cell_type": "code",
   "id": "abaac4b6",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for idx, patch_feats in enumerate(vision_patches):\n",
    "    pca2 = PCA(n_components=2, random_state=0).fit_transform(patch_feats)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.title(f\"DINOv2 Patch Tokens PCA 2D (Image {idx})\")\n",
    "    plt.scatter(pca2[:,0], pca2[:,1], s=10, alpha=0.7)\n",
    "    plt.xlabel(\"PC1\")\n",
    "    plt.ylabel(\"PC2\")\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a975ba31",
   "metadata": {},
   "source": [
    "\n",
    "**Checkpoint Q2 (short answer):**  \n",
    "In your PCA‑RGB patch maps, what visual regions (e.g., edges, textures, objects) appear grouped or contrasted along the first 1–2 components? Why might DINOv2 emphasize those?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7a448f",
   "metadata": {},
   "source": [
    "## 3) Audio: Wav2Vec2‑BERT Embeddings & PCA (35 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ceada48",
   "metadata": {},
   "source": [
    "\n",
    "### 3.1 Load Audio\n",
    "Use a short WAV (16 kHz). You can:\n",
    "- Use the example URL below, or\n",
    "- Replace with your own `.wav` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "6f383c7f",
   "metadata": {},
   "source": [
    "\n",
    "import librosa, soundfile as sf, numpy as np, requests, io, os\n",
    "\n",
    "# Example sample (replace with your own if preferred)\n",
    "AUDIO_URL = \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac?download=true\"\n",
    "\n",
    "def load_audio_from_url(url, target_sr=16000):\n",
    "    r = requests.get(url, timeout=10)\n",
    "    r.raise_for_status()\n",
    "    data, sr = sf.read(io.BytesIO(r.content))\n",
    "    if data.ndim > 1:\n",
    "        data = np.mean(data, axis=1)\n",
    "    if sr != target_sr:\n",
    "        data = librosa.resample(data, orig_sr=sr, target_sr=target_sr)\n",
    "        sr = target_sr\n",
    "    return data.astype(np.float32), sr\n",
    "\n",
    "audio, sr = load_audio_from_url(AUDIO_URL)\n",
    "print(\"Audio length (s):\", len(audio)/sr, \"| sr:\", sr)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c4643c71",
   "metadata": {},
   "source": [
    "### 3.2 Load Wav2Vec2‑BERT model & processor"
   ]
  },
  {
   "cell_type": "code",
   "id": "b5367bd6",
   "metadata": {},
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# TODO: Load \"facebook/wav2vec2-base-960h\" and assigne it to audio_model\n",
    "MODEL_ID_AUDIO = \"facebook/wav2vec2-base-960h\"  # you can try XLSR for multilingual\n",
    "feat_extractor = AutoFeatureExtractor.from_pretrained(MODEL_ID_AUDIO)\n",
    "audio_model = AutoModel.from_pretrained(MODEL_ID_AUDIO).to(device).eval()\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a49f6faf",
   "metadata": {},
   "source": [
    "### 3.3 Extract Frame-Level Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "id": "62d10ab5",
   "metadata": {},
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def get_wav2vec2_features(wav):\n",
    "    #TO DO: Write the full extraction function\n",
    "    return last_hidden  # [frames, dim]\n",
    "\n",
    "audio_feats = get_wav2vec2_features(audio)\n",
    "audio_feats.shape\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2d44af9d",
   "metadata": {},
   "source": [
    "### 3.4 PCA → 2D/3D & Time Plot"
   ]
  },
  {
   "cell_type": "code",
   "id": "2af9476a",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pca2 = PCA(n_components=2, random_state=0).fit_transform(audio_feats)\n",
    "pca3 = PCA(n_components=3, random_state=0).fit_transform(audio_feats)\n",
    "\n",
    "# 2D scatter colored by time\n",
    "t = np.arange(len(pca2))\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.title(\"Wav2Vec2 Frame PCA (2D) — colored by time\")\n",
    "plt.scatter(pca2[:,0], pca2[:,1], c=t, s=5)\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.colorbar(label=\"frame index\")\n",
    "plt.show()\n",
    "\n",
    "# Each PC over time\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.title(\"First 3 PCs over time\")\n",
    "for i in range(3):\n",
    "    plt.plot(pca3[:, i], label=f\"PC{i+1}\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Frame index\")\n",
    "plt.ylabel(\"Component value\")\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c8ee341",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Compare & Reflect\n",
    "**Checkpoint Q3:**  \n",
    "Compare PC1–PC3 between vision (patch tokens) and audio (frame features).  \n",
    "- What kinds of structure do you see (e.g., edges/regions vs. phonetic/energy changes)?  \n",
    "- How do augmentations in SSL pretraining encourage such separations?\n",
    "\n",
    "**Stretch Thought:**  \n",
    "If you swapped PCA for t‑SNE or UMAP, what tradeoffs would you expect? When would PCA be preferable?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72e4a299",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Troubleshooting & Tips\n",
    "- **Slow downloads**: Hugging Face models cache after first use. Use a stable internet connection.\n",
    "- **CUDA OOM**: Try smaller models (e.g., `facebook/dinov2-small`) or reduce batch sizes / image count.\n",
    "- **Different sampling rates**: Always resample audio to 16 kHz for Wav2Vec2.\n",
    "- **Reproducibility**: Set `random_state=0` in PCA; seed torch/np if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59da80be",
   "metadata": {},
   "source": [
    "\n",
    "## 6) Deliverables (submit with this notebook)\n",
    "1. A PCA‑RGB patch map for at least **two** images.\n",
    "2. One **2D patch-token scatter** for one image.\n",
    "3. A **2D/3D PCA visualization** for audio frames and a **PCs-over-time** plot.\n",
    "4. Brief answers to **Q1–Q3** (2–3 sentences each).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190becf",
   "metadata": {},
   "source": [
    "\n",
    "## 7) (Bonus) Experiments if you have time\n",
    "- Try another DINOv2 size (`small`, `large`) and note differences.\n",
    "- Swap audio model to `facebook/wav2vec2-base` vs. `wav2vec2-large-robust` and compare PCA spread.\n",
    "- Pooling strategies: CLS vs. mean of patches/frames; layer-wise comparisons.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
